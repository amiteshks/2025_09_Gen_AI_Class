import os
from dotenv import load_dotenv
from openai import OpenAI
from sentence_transformers import SentenceTransformer

from pymilvus import (
    connections,
    Collection,
    CollectionSchema,
    FieldSchema,
    DataType,
    utility,
)


# ---------------------------
# Semantic Search Function
# ---------------------------

def search_similar(query, collection_name="employee_policies", top_k=1):
    """
    Given a user query, return top K semantically similar texts from Milvus.
    """
    connections.connect("default", host="localhost", port="19530")

    collection = Collection(collection_name)
    model = SentenceTransformer("all-MiniLM-L6-v2")

    query_vector = model.encode([query]).tolist()

    results = collection.search(
        data=query_vector,
        anns_field="embedding",
        param={"metric_type": "COSINE", "params": {"nprobe": 10}},
        limit=top_k,
        output_fields=["content"]
    )

    top_docs = []
    for hit in results[0]:
        top_docs.append({
            "content": hit.entity.get("content"),
            "score": hit.distance
        })

    return top_docs


# ---------------------------
# LLM Answer Generation
# ---------------------------

def generate_answer(query, contexts):
    """
    Generate an answer using OpenAI GPT model based on retrieved contexts.
    """
    load_dotenv(override=True, dotenv_path="../.env")
    my_api_key = os.getenv("OPEN_AI_API_KEY")

    client = OpenAI(api_key=my_api_key)


    context_str = "\n".join(contexts)
    prompt = f"Context:\n{context_str}\n\nQuestion: {query}\nAnswer:"

    response = client.chat.completions.create(
        model="gpt-5-nano",  # or gpt-4o if you have access
        messages=[
            {"role": "system", "content": "You are a helpful assistant that answers based only on context."},
            {"role": "user", "content": prompt}
        ]
    )

    return response.choices[0].message.content.strip()


# ---------------------------
# Combined RAG Query Pipeline
# ---------------------------

def retrieve_and_generate_response(query):
    """
    Perform full RAG flow: search + LLM answer generation.
    """
    top_docs = search_similar(query)
    contexts = [doc["content"] for doc in top_docs]
    answer = generate_answer(query, contexts)

    print(" Retrieved Contexts:")
    for i, c in enumerate(contexts, start=1):
        print(f"{i}. {c}")

    print("\n LLM Answer:")
    print(answer)

    return {
        "query": query,
        "contexts": contexts,
        "answer": answer
    }


# ---------------------------
# 5. Example Run
# ---------------------------
if __name__ == "__main__":
    
    query = "How often do employees get paid?"
    retrieve_and_generate_response(query)
